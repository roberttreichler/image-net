{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "#from IPython.display import Image, display\n",
    "\n",
    "import PIL\n",
    "#from PIL import Image\n",
    "\n",
    "# Functions and classes for loading and using the Inception model.\n",
    "#import inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_DIR = \"c:\\\\Users\\\\rtreichl\\\\Documents\\\\cat_do\\\\train\\\\\"\n",
    "TEST_DIR  = 'c:\\\\Users\\\\rtreichl\\\\Documents\\\\cat_do\\\\test\\\\'\n",
    "# On the kaggle notebook\n",
    "# we only take the first 2000 from the training set\n",
    "# and only the first 1000 from the test set\n",
    "# REMOVE [0:2000] and [0:1000] when running locally\n",
    "train_image_file_names = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)]\n",
    "random.shuffle(train_image_file_names)\n",
    "train_image_file_names = train_image_file_names[:2000]\n",
    "#test_image_file_names = [TEST_DIR+i for i in os.listdir(TEST_DIR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slow, yet simple implementation with tensorflow\n",
    "# could be rewritten to be much faster\n",
    "# (which is not really needed as it takes less than 5 minutes on my laptop)\n",
    "def decode_image(image_file_names, resize_func=None):\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        file_name = tf.placeholder(dtype=tf.string)\n",
    "        file = tf.read_file(file_name)\n",
    "        image = tf.image.decode_jpeg(file)\n",
    "        if resize_func != None:\n",
    "            image = resize_func(image)\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()   \n",
    "        for i in range(len(image_file_names)):\n",
    "            images.append(session.run(image, feed_dict={file_name: image_file_names[i]}))\n",
    "            if (i+1) % 1000 == 0:\n",
    "                print('Images processed: ',i+1)\n",
    "        \n",
    "        session.close()\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-bd49dae09433>:17: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Images processed:  1000\n",
      "Images processed:  2000\n"
     ]
    }
   ],
   "source": [
    "train_images = decode_image(train_image_file_names)\n",
    "#test_images = decode_image(test_image_file_names)\n",
    "#all_images = train_images + test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIDTH=64\n",
    "HEIGHT=64\n",
    "resize_func = lambda image: tf.image.resize_image_with_crop_or_pad(image, HEIGHT, WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-bd49dae09433>:17: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Images processed:  1000\n",
      "Images processed:  2000\n"
     ]
    }
   ],
   "source": [
    "processed_train_images = decode_image(train_image_file_names, resize_func=resize_func)\n",
    "#processed_test_images = decode_image(test_image_file_names, resize_func=resize_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# Chech the shapes\n",
    "print(np.shape(processed_train_images))\n",
    "#print(np.shape(processed_test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 64, 64, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(processed_train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = [[1., 0.] if 'dog' in name else [0., 1.] for name in train_image_file_names]\n",
    "X_train=np.reshape(processed_train_images,(np.shape(processed_train_images)[0],HEIGHT*WIDTH*3))\n",
    "X_train=X_train.astype(\"float32\")\n",
    "y_train=labels\n",
    "#y_train=labels.astype(\"float32\")\n",
    "y_train=pd.DataFrame(y_train)\n",
    "y_train=y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    1038\n",
       "0.0     962\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_train)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\monitors.py:322: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n"
     ]
    }
   ],
   "source": [
    "validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(X_train,y_train,every_n_steps=100,\n",
    "                                                                 early_stopping_metric=\"accuracy\",\n",
    "                                                                 early_stopping_metric_minimize=False,\n",
    "                                                                 early_stopping_rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\rtreichl\\AppData\\Local\\Temp\\tmprk6pi_6p\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 600, '_task_type': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_ps_replicas': 0, '_tf_random_seed': None, '_task_id': 0, '_master': '', '_save_checkpoints_steps': None, '_is_chief': True, '_environment': 'local', '_evaluation_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000000000CF80E80>}\n"
     ]
    }
   ],
   "source": [
    "# Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "classifier = tf.contrib.learn.DNNClassifier(feature_columns=[tf.contrib.layers.real_valued_column(\"\", dimension=X_train.shape[0])],\n",
    "                                            hidden_units=[64,32,16],\n",
    "                                            n_classes=2,\n",
    "                                            dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-3b49ad86cc32>:2: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-28-3b49ad86cc32>:2: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:From C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  equality = a == b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\rtreichl\\AppData\\Local\\Temp\\tmprk6pi_6p\\model.ckpt.\n",
      "INFO:tensorflow:loss = 109.953, step = 1\n",
      "WARNING:tensorflow:From C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\monitors.py:712: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\monitors.py:712: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:From C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-03-03-00:29:40\n",
      "INFO:tensorflow:Finished evaluation at 2017-03-03-00:29:41\n",
      "INFO:tensorflow:Saving dict for global step 1: accuracy = 0.481, accuracy/baseline_label_mean = 0.519, accuracy/threshold_0.500000_mean = 0.481, auc = 0.5, global_step = 1, labels/actual_label_mean = 0.519, labels/prediction_mean = 0.0, loss = 1338.7, precision/positive_threshold_0.500000_mean = 0.0, recall/positive_threshold_0.500000_mean = 0.0\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 100): labels/prediction_mean = 0.0, auc = 0.5, precision/positive_threshold_0.500000_mean = 0.0, accuracy/baseline_label_mean = 0.519, recall/positive_threshold_0.500000_mean = 0.0, loss = 1338.7, accuracy/threshold_0.500000_mean = 0.481, global_step = 1, labels/actual_label_mean = 0.519, accuracy = 0.481\n",
      "INFO:tensorflow:global_step/sec: 5.9544\n",
      "INFO:tensorflow:loss = 0.696154, step = 101\n",
      "INFO:tensorflow:global_step/sec: 6.94618\n",
      "INFO:tensorflow:loss = 0.693723, step = 201\n",
      "INFO:tensorflow:global_step/sec: 7.03314\n",
      "INFO:tensorflow:loss = 0.69259, step = 301\n",
      "INFO:tensorflow:global_step/sec: 7.31112\n",
      "INFO:tensorflow:loss = 0.692222, step = 401\n",
      "INFO:tensorflow:global_step/sec: 7.34247\n",
      "INFO:tensorflow:loss = 0.692663, step = 501\n",
      "INFO:tensorflow:global_step/sec: 7.32279\n",
      "INFO:tensorflow:loss = 0.692109, step = 601\n",
      "INFO:tensorflow:global_step/sec: 7.31979\n",
      "INFO:tensorflow:loss = 0.692223, step = 701\n",
      "INFO:tensorflow:global_step/sec: 7.36084\n",
      "INFO:tensorflow:loss = 0.692322, step = 801\n",
      "INFO:tensorflow:global_step/sec: 7.35283\n",
      "INFO:tensorflow:loss = 0.692375, step = 901\n",
      "INFO:tensorflow:global_step/sec: 7.15748\n",
      "INFO:tensorflow:loss = 0.692182, step = 1001\n",
      "INFO:tensorflow:global_step/sec: 7.40763\n",
      "INFO:tensorflow:loss = 0.692919, step = 1101\n",
      "INFO:tensorflow:global_step/sec: 7.35067\n",
      "INFO:tensorflow:loss = 0.693769, step = 1201\n",
      "INFO:tensorflow:global_step/sec: 7.3993\n",
      "INFO:tensorflow:loss = 0.692669, step = 1301\n",
      "INFO:tensorflow:global_step/sec: 7.41367\n",
      "INFO:tensorflow:loss = 0.691595, step = 1401\n",
      "INFO:tensorflow:global_step/sec: 7.39984\n",
      "INFO:tensorflow:loss = 0.692182, step = 1501\n",
      "INFO:tensorflow:global_step/sec: 7.27558\n",
      "INFO:tensorflow:loss = 0.692119, step = 1601\n",
      "INFO:tensorflow:global_step/sec: 7.09391\n",
      "INFO:tensorflow:loss = 0.692143, step = 1701\n",
      "INFO:tensorflow:global_step/sec: 6.81171\n",
      "INFO:tensorflow:loss = 0.69302, step = 1801\n",
      "INFO:tensorflow:global_step/sec: 6.72531\n",
      "INFO:tensorflow:loss = 0.692084, step = 1901\n",
      "INFO:tensorflow:global_step/sec: 6.83116\n",
      "INFO:tensorflow:loss = 0.693294, step = 2001\n",
      "INFO:tensorflow:global_step/sec: 7.11774\n",
      "INFO:tensorflow:loss = 0.692379, step = 2101\n",
      "INFO:tensorflow:global_step/sec: 6.72061\n",
      "INFO:tensorflow:loss = 0.692153, step = 2201\n",
      "INFO:tensorflow:global_step/sec: 6.81793\n",
      "INFO:tensorflow:loss = 0.692487, step = 2301\n",
      "INFO:tensorflow:global_step/sec: 7.33084\n",
      "INFO:tensorflow:loss = 0.692715, step = 2401\n",
      "INFO:tensorflow:global_step/sec: 7.30023\n",
      "INFO:tensorflow:loss = 0.692133, step = 2501\n",
      "INFO:tensorflow:global_step/sec: 7.35078\n",
      "INFO:tensorflow:loss = 0.692741, step = 2601\n",
      "INFO:tensorflow:global_step/sec: 7.12616\n",
      "INFO:tensorflow:loss = 0.692149, step = 2701\n",
      "INFO:tensorflow:global_step/sec: 7.28948\n",
      "INFO:tensorflow:loss = 0.691944, step = 2801\n",
      "INFO:tensorflow:global_step/sec: 7.31786\n",
      "INFO:tensorflow:loss = 0.692334, step = 2901\n",
      "INFO:tensorflow:global_step/sec: 7.30236\n",
      "INFO:tensorflow:loss = 0.692132, step = 3001\n",
      "INFO:tensorflow:global_step/sec: 7.37964\n",
      "INFO:tensorflow:loss = 0.692528, step = 3101\n",
      "INFO:tensorflow:global_step/sec: 7.33665\n",
      "INFO:tensorflow:loss = 0.692357, step = 3201\n",
      "INFO:tensorflow:global_step/sec: 7.3288\n",
      "INFO:tensorflow:loss = 0.69266, step = 3301\n",
      "INFO:tensorflow:global_step/sec: 7.28842\n",
      "INFO:tensorflow:loss = 0.69216, step = 3401\n",
      "INFO:tensorflow:global_step/sec: 7.33407\n",
      "INFO:tensorflow:loss = 0.692163, step = 3501\n",
      "INFO:tensorflow:global_step/sec: 7.17844\n",
      "INFO:tensorflow:loss = 0.69245, step = 3601\n",
      "INFO:tensorflow:global_step/sec: 7.31764\n",
      "INFO:tensorflow:loss = 0.692461, step = 3701\n",
      "INFO:tensorflow:global_step/sec: 7.4667\n",
      "INFO:tensorflow:loss = 0.691895, step = 3801\n",
      "INFO:tensorflow:global_step/sec: 7.41897\n",
      "INFO:tensorflow:loss = 0.692207, step = 3901\n",
      "INFO:tensorflow:global_step/sec: 7.28326\n",
      "INFO:tensorflow:loss = 0.692598, step = 4001\n",
      "INFO:tensorflow:global_step/sec: 7.46775\n",
      "INFO:tensorflow:loss = 0.692309, step = 4101\n",
      "INFO:tensorflow:global_step/sec: 7.47422\n",
      "INFO:tensorflow:loss = 0.69257, step = 4201\n",
      "INFO:tensorflow:global_step/sec: 7.44418\n",
      "INFO:tensorflow:loss = 0.692573, step = 4301\n",
      "INFO:tensorflow:Saving checkpoints for 4318 into C:\\Users\\rtreichl\\AppData\\Local\\Temp\\tmprk6pi_6p\\model.ckpt.\n",
      "WARNING:tensorflow:From C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\monitors.py:712: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\monitors.py:712: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:From C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-03-03-00:39:36\n",
      "INFO:tensorflow:Finished evaluation at 2017-03-03-00:39:37\n",
      "INFO:tensorflow:Saving dict for global step 4318: accuracy = 0.519, accuracy/baseline_label_mean = 0.519, accuracy/threshold_0.500000_mean = 0.519, auc = 0.514396, global_step = 4318, labels/actual_label_mean = 0.519, labels/prediction_mean = 0.51878, loss = 0.692181, precision/positive_threshold_0.500000_mean = 0.519, recall/positive_threshold_0.500000_mean = 1.0\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 4400): labels/prediction_mean = 0.51878, auc = 0.514396, precision/positive_threshold_0.500000_mean = 0.519, accuracy/baseline_label_mean = 0.519, recall/positive_threshold_0.500000_mean = 1.0, loss = 0.692181, accuracy/threshold_0.500000_mean = 0.519, global_step = 4318, labels/actual_label_mean = 0.519, accuracy = 0.519\n",
      "INFO:tensorflow:global_step/sec: 6.59\n",
      "INFO:tensorflow:loss = 0.691814, step = 4401\n",
      "INFO:tensorflow:global_step/sec: 7.53096\n",
      "INFO:tensorflow:loss = 0.692968, step = 4501\n",
      "INFO:tensorflow:global_step/sec: 7.46574\n",
      "INFO:tensorflow:loss = 0.692224, step = 4601\n",
      "INFO:tensorflow:global_step/sec: 7.5141\n",
      "INFO:tensorflow:loss = 0.691551, step = 4701\n",
      "INFO:tensorflow:global_step/sec: 7.5026\n",
      "INFO:tensorflow:loss = 0.692303, step = 4801\n",
      "INFO:tensorflow:global_step/sec: 7.48975\n",
      "INFO:tensorflow:loss = 0.692616, step = 4901\n",
      "INFO:tensorflow:global_step/sec: 7.5117\n",
      "INFO:tensorflow:loss = 0.692462, step = 5001\n",
      "INFO:tensorflow:global_step/sec: 6.98696\n",
      "INFO:tensorflow:loss = 0.692209, step = 5101\n",
      "INFO:tensorflow:global_step/sec: 7.44902\n",
      "INFO:tensorflow:loss = 0.691889, step = 5201\n",
      "INFO:tensorflow:global_step/sec: 7.44425\n",
      "INFO:tensorflow:loss = 0.692252, step = 5301\n",
      "INFO:tensorflow:global_step/sec: 7.41003\n",
      "INFO:tensorflow:loss = 0.692225, step = 5401\n",
      "INFO:tensorflow:global_step/sec: 6.95496\n",
      "INFO:tensorflow:loss = 0.692423, step = 5501\n",
      "INFO:tensorflow:global_step/sec: 7.10557\n",
      "INFO:tensorflow:loss = 0.692342, step = 5601\n",
      "INFO:tensorflow:global_step/sec: 7.24455\n",
      "INFO:tensorflow:loss = 0.692277, step = 5701\n",
      "INFO:tensorflow:global_step/sec: 7.46682\n",
      "INFO:tensorflow:loss = 0.692906, step = 5801\n",
      "INFO:tensorflow:global_step/sec: 7.25596\n",
      "INFO:tensorflow:loss = 0.692325, step = 5901\n",
      "INFO:tensorflow:global_step/sec: 7.06888\n",
      "INFO:tensorflow:loss = 0.691808, step = 6001\n",
      "INFO:tensorflow:global_step/sec: 7.44924\n",
      "INFO:tensorflow:loss = 0.692548, step = 6101\n",
      "INFO:tensorflow:global_step/sec: 7.47307\n",
      "INFO:tensorflow:loss = 0.69315, step = 6201\n",
      "INFO:tensorflow:global_step/sec: 7.47162\n",
      "INFO:tensorflow:loss = 0.692652, step = 6301\n",
      "INFO:tensorflow:global_step/sec: 7.23848\n",
      "INFO:tensorflow:loss = 0.691979, step = 6401\n",
      "INFO:tensorflow:global_step/sec: 7.26573\n",
      "INFO:tensorflow:loss = 0.692118, step = 6501\n",
      "INFO:tensorflow:global_step/sec: 7.4762\n",
      "INFO:tensorflow:loss = 0.692273, step = 6601\n",
      "INFO:tensorflow:global_step/sec: 7.47296\n",
      "INFO:tensorflow:loss = 0.69186, step = 6701\n",
      "INFO:tensorflow:global_step/sec: 7.46426\n",
      "INFO:tensorflow:loss = 0.692016, step = 6801\n",
      "INFO:tensorflow:global_step/sec: 7.48684\n",
      "INFO:tensorflow:loss = 0.692418, step = 6901\n",
      "INFO:tensorflow:global_step/sec: 7.38311\n",
      "INFO:tensorflow:loss = 0.69236, step = 7001\n",
      "INFO:tensorflow:global_step/sec: 6.60258\n",
      "INFO:tensorflow:loss = 0.691967, step = 7101\n",
      "INFO:tensorflow:global_step/sec: 6.94803\n",
      "INFO:tensorflow:loss = 0.692184, step = 7201\n",
      "INFO:tensorflow:global_step/sec: 7.14141\n",
      "INFO:tensorflow:loss = 0.691942, step = 7301\n",
      "INFO:tensorflow:global_step/sec: 7.03444\n",
      "INFO:tensorflow:loss = 0.692239, step = 7401\n",
      "INFO:tensorflow:global_step/sec: 7.41074\n",
      "INFO:tensorflow:loss = 0.692394, step = 7501\n",
      "INFO:tensorflow:global_step/sec: 7.50691\n",
      "INFO:tensorflow:loss = 0.692248, step = 7601\n",
      "INFO:tensorflow:global_step/sec: 7.17394\n",
      "INFO:tensorflow:loss = 0.691692, step = 7701\n",
      "INFO:tensorflow:global_step/sec: 7.03633\n",
      "INFO:tensorflow:loss = 0.692039, step = 7801\n",
      "INFO:tensorflow:global_step/sec: 6.9295\n",
      "INFO:tensorflow:loss = 0.69234, step = 7901\n",
      "INFO:tensorflow:global_step/sec: 7.01093\n",
      "INFO:tensorflow:loss = 0.691896, step = 8001\n",
      "INFO:tensorflow:global_step/sec: 7.47258\n",
      "INFO:tensorflow:loss = 0.692712, step = 8101\n",
      "INFO:tensorflow:global_step/sec: 7.42067\n",
      "INFO:tensorflow:loss = 0.691331, step = 8201\n",
      "INFO:tensorflow:global_step/sec: 7.47526\n",
      "INFO:tensorflow:loss = 0.692125, step = 8301\n",
      "INFO:tensorflow:global_step/sec: 7.31917\n",
      "INFO:tensorflow:loss = 0.69272, step = 8401\n",
      "INFO:tensorflow:global_step/sec: 7.29413\n",
      "INFO:tensorflow:loss = 0.692329, step = 8501\n",
      "INFO:tensorflow:global_step/sec: 7.47012\n",
      "INFO:tensorflow:loss = 0.692905, step = 8601\n",
      "INFO:tensorflow:Saving checkpoints for 8688 into C:\\Users\\rtreichl\\AppData\\Local\\Temp\\tmprk6pi_6p\\model.ckpt.\n",
      "WARNING:tensorflow:From C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\monitors.py:712: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\monitors.py:712: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:From C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-03-03-00:49:27\n",
      "INFO:tensorflow:Finished evaluation at 2017-03-03-00:49:28\n",
      "INFO:tensorflow:Saving dict for global step 8688: accuracy = 0.519, accuracy/baseline_label_mean = 0.519, accuracy/threshold_0.500000_mean = 0.519, auc = 0.516242, global_step = 8688, labels/actual_label_mean = 0.519, labels/prediction_mean = 0.518927, loss = 0.692086, precision/positive_threshold_0.500000_mean = 0.519, recall/positive_threshold_0.500000_mean = 1.0\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 8700): labels/prediction_mean = 0.518927, auc = 0.516242, precision/positive_threshold_0.500000_mean = 0.519, accuracy/baseline_label_mean = 0.519, recall/positive_threshold_0.500000_mean = 1.0, loss = 0.692086, accuracy/threshold_0.500000_mean = 0.519, global_step = 8688, labels/actual_label_mean = 0.519, accuracy = 0.519\n",
      "INFO:tensorflow:Stopping. Best step: 4400 with accuracy = 0.5189999938011169.\n",
      "INFO:tensorflow:Saving checkpoints for 8700 into C:\\Users\\rtreichl\\AppData\\Local\\Temp\\tmprk6pi_6p\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.692642.\n",
      "WARNING:tensorflow:From <ipython-input-28-3b49ad86cc32>:4: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-28-3b49ad86cc32>:4: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:From C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-03-03-00:49:29\n",
      "INFO:tensorflow:Finished evaluation at 2017-03-03-00:49:30\n",
      "INFO:tensorflow:Saving dict for global step 8700: accuracy = 0.519, accuracy/baseline_label_mean = 0.519, accuracy/threshold_0.500000_mean = 0.519, auc = 0.516314, global_step = 8700, labels/actual_label_mean = 0.519, labels/prediction_mean = 0.518969, loss = 0.692084, precision/positive_threshold_0.500000_mean = 0.519, recall/positive_threshold_0.500000_mean = 1.0\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "Accuracy: 0.519000\n",
      "WARNING:tensorflow:From C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn.py:374: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3b49ad86cc32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0my_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDNNClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m               \u001b[0m_call_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m               func.__module__, arg_name, arg_value, date, instructions)\n\u001b[0;32m--> 334\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\n\u001b[1;32m    336\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32mC:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, x, input_fn, batch_size, as_iterable)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \"\"\"\n\u001b[1;32m    403\u001b[0m     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction_key\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPredictionKey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPROBABILITIES\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m     preds = super(DNNClassifier, self).predict(\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "classifier.fit(x=X_train, y=y_train, steps=100000,monitors=[validation_monitor])\n",
    "\n",
    "accuracy_score = classifier.evaluate(x=X_train, y=y_train)[\"accuracy\"]\n",
    "print('Accuracy: {0:f}'.format(accuracy_score))\n",
    "\n",
    "y = list(classifier.predict(X_train))\n",
    "y_prob=list(classifier.predict_proba(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn.py:374: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  equality = a == b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rtreichl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn.py:409: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    }
   ],
   "source": [
    "y = list(classifier.predict(X_train))\n",
    "y_prob=list(classifier.predict_proba(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2000.0\n",
       "mean        1.0\n",
       "std         0.0\n",
       "min         1.0\n",
       "25%         1.0\n",
       "50%         1.0\n",
       "75%         1.0\n",
       "max         1.0\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y)[0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setup a standard image size; this will distort some images but will get everything into the same shape\n",
    "STANDARD_SIZE = (64, 64)\n",
    "def img_to_matrix(filename, verbose=False):\n",
    "    \"\"\"\n",
    "    takes a filename and turns it into a numpy array of RGB pixels\n",
    "    \"\"\"\n",
    "    img = PIL.Image.open(filename)\n",
    "    if verbose==True:\n",
    "        print (\"changing size from %s to %s\" % (str(img.size), str(STANDARD_SIZE)))\n",
    "    img = img.resize((250,250),PIL.Image.ANTIALIAS)\n",
    "    img = list(img.getdata())\n",
    "    #img = map(list, img)\n",
    "    img = [list(row) for row in img]\n",
    "    img = np.array(img)\n",
    "    return img\n",
    "\n",
    "def flatten_image(img):\n",
    "    \"\"\"\n",
    "    takes in an (m, n) numpy array and flattens it \n",
    "    into an array of shape (1, m * n)\n",
    "    \"\"\"\n",
    "    s = img.shape[0] * img.shape[1]\n",
    "    img_wide = img.reshape(1, s)\n",
    "    return img_wide[0]\n",
    "\n",
    "img_dir = \"/home/rogue1/Documents/cats_dogs/train/\"\n",
    "images = [img_dir+ f for f in os.listdir(img_dir)]\n",
    "labels = [\"dog\" if \"dog\" in f.split('/')[-1] else \"cat\" for f in images]\n",
    "\n",
    "data = []\n",
    "for image in images:\n",
    "    img = img_to_matrix(image)\n",
    "    img = flatten_image(img)\n",
    "    data.append(img)\n",
    "\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Binary data\n",
    "np.save('data.npy', data)\n",
    "\n",
    "#Human readable data\n",
    "np.savetxt('data.txt', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***********************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verbose=False\n",
    "filename='/home/rogue1/Documents/cats_dogs/train/cat.6319.jpg'\n",
    "img = PIL.Image.open(filename)\n",
    "if verbose==True:\n",
    "    print (\"changing size from %s to %s\" % (str(img.size), str(STANDARD_SIZE)))\n",
    "img = img.resize(STANDARD_SIZE)\n",
    "img = list(img.getdata())\n",
    "#img = map(list, img)\n",
    "#img = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = img.shape[0] * img.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flist=os.listdir('/home/rogue1/Documents/cats_dogs/train')\n",
    "\n",
    "id_=[]\n",
    "imgarr_=np.array([])\n",
    "imgarr_= imgarr_.resize((250,250))\n",
    "for fl in flist:\n",
    "    image_path = '/home/rogue1/Documents/cats_dogs/train/' + fl\n",
    "    img = PIL.Image.open(image_path).convert(\"RGB\")\n",
    "    img = img.resize((250,250),PIL.Image.ANTIALIAS)\n",
    "    imgarr = np.array(img)\n",
    "    imgarr_ = np.concatenate((imgarr,imgarr_),axis=1)   \n",
    "    id_.append(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgarr_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(imgarr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgarr_=np.array(imgarr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(imgarr_[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file='/home/rogue1/Documents/cats_dogs/train/cat.0.jpg'\n",
    "\n",
    "image_path = '/home/rogue1/Documents/cats_dogs/train/cat.0.jpg'\n",
    "img = PIL.Image.open(image_path).convert(\"RGB\")\n",
    "img = img.resize((250,250),PIL.Image.ANTIALIAS)\n",
    "imgarr = np.array(img).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(imgarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img2=np.ravel(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directory to store the downloaded data.\n",
    "data_dir = \"inception/\"\n",
    "\n",
    "# File containing the mappings between class-number and uid. (Downloaded)\n",
    "path_uid_to_cls = \"imagenet_2012_challenge_label_map_proto.pbtxt\"\n",
    "\n",
    "# File containing the mappings between uid and string. (Downloaded)\n",
    "path_uid_to_name = \"imagenet_synset_to_human_label_map.txt\"\n",
    "\n",
    "# File containing the TensorFlow graph definition. (Downloaded)\n",
    "path_graph_def = \"classify_image_graph_def.pb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Inception:\n",
    "    \"\"\"\n",
    "    The Inception model is a Deep Neural Network which has already been\n",
    "    trained for classifying images into 1000 different categories.\n",
    "\n",
    "    When you create a new instance of this class, the Inception model\n",
    "    will be loaded and can be used immediately without training.\n",
    "\n",
    "    The Inception model can also be used for Transfer Learning.\n",
    "    \"\"\"\n",
    "\n",
    "    # Name of the tensor for feeding the input image as jpeg.\n",
    "    tensor_name_input_jpeg = \"DecodeJpeg/contents:0\"\n",
    "\n",
    "    # Name of the tensor for feeding the decoded input image.\n",
    "    # Use this for feeding images in other formats than jpeg.\n",
    "    tensor_name_input_image = \"DecodeJpeg:0\"\n",
    "\n",
    "    # Name of the tensor for the resized input image.\n",
    "    # This is used to retrieve the image after it has been resized.\n",
    "    tensor_name_resized_image = \"ResizeBilinear:0\"\n",
    "\n",
    "    # Name of the tensor for the output of the softmax-classifier.\n",
    "    # This is used for classifying images with the Inception model.\n",
    "    tensor_name_softmax = \"softmax:0\"\n",
    "\n",
    "    # Name of the tensor for the unscaled outputs of the softmax-classifier (aka. logits).\n",
    "    tensor_name_softmax_logits = \"softmax/logits:0\"\n",
    "\n",
    "    # Name of the tensor for the output of the Inception model.\n",
    "    # This is used for Transfer Learning.\n",
    "    tensor_name_transfer_layer = \"pool_3:0\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Mappings between class-numbers and class-names.\n",
    "        # Used to print the class-name as a string e.g. \"horse\" or \"plant\".\n",
    "        self.name_lookup = NameLookup()\n",
    "\n",
    "        # Now load the Inception model from file. The way TensorFlow\n",
    "        # does this is confusing and requires several steps.\n",
    "\n",
    "        # Create a new TensorFlow computational graph.\n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "        # Set the new graph as the default.\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # TensorFlow graphs are saved to disk as so-called Protocol Buffers\n",
    "            # aka. proto-bufs which is a file-format that works on multiple\n",
    "            # platforms. In this case it is saved as a binary file.\n",
    "\n",
    "            # Open the graph-def file for binary reading.\n",
    "            path = os.path.join(data_dir, path_graph_def)\n",
    "            with tf.gfile.FastGFile(path, 'rb') as file:\n",
    "                # The graph-def is a saved copy of a TensorFlow graph.\n",
    "                # First we need to create an empty graph-def.\n",
    "                graph_def = tf.GraphDef()\n",
    "\n",
    "                # Then we load the proto-buf file into the graph-def.\n",
    "                graph_def.ParseFromString(file.read())\n",
    "\n",
    "                # Finally we import the graph-def to the default TensorFlow graph.\n",
    "                tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "                # Now self.graph holds the Inception model from the proto-buf file.\n",
    "\n",
    "        # Get the output of the Inception model by looking up the tensor\n",
    "        # with the appropriate name for the output of the softmax-classifier.\n",
    "        self.y_pred = self.graph.get_tensor_by_name(self.tensor_name_softmax)\n",
    "\n",
    "        # Get the unscaled outputs for the Inception model (aka. softmax-logits).\n",
    "        self.y_logits = self.graph.get_tensor_by_name(self.tensor_name_softmax_logits)\n",
    "\n",
    "        # Get the tensor for the resized image that is input to the neural network.\n",
    "        self.resized_image = self.graph.get_tensor_by_name(self.tensor_name_resized_image)\n",
    "\n",
    "        # Get the tensor for the last layer of the graph, aka. the transfer-layer.\n",
    "        self.transfer_layer = self.graph.get_tensor_by_name(self.tensor_name_transfer_layer)\n",
    "\n",
    "        # Get the number of elements in the transfer-layer.\n",
    "        self.transfer_len = self.transfer_layer.get_shape()[3]\n",
    "\n",
    "        # Create a TensorFlow session for executing the graph.\n",
    "        self.session = tf.Session(graph=self.graph)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Call this function when you are done using the Inception model.\n",
    "        It closes the TensorFlow session to release its resources.\n",
    "        \"\"\"\n",
    "\n",
    "        self.session.close()\n",
    "\n",
    "    def _write_summary(self, logdir='summary/'):\n",
    "        \"\"\"\n",
    "        Write graph to summary-file so it can be shown in TensorBoard.\n",
    "\n",
    "        This function is used for debugging and may be changed or removed in the future.\n",
    "\n",
    "        :param logdir:\n",
    "            Directory for writing the summary-files.\n",
    "\n",
    "        :return:\n",
    "            Nothing.\n",
    "        \"\"\"\n",
    "\n",
    "        writer = tf.train.SummaryWriter(logdir=logdir, graph=self.graph)\n",
    "        writer.close()\n",
    "\n",
    "    def _create_feed_dict(self, image_path=None, image=None):\n",
    "        \"\"\"\n",
    "        Create and return a feed-dict with an image.\n",
    "\n",
    "        :param image_path:\n",
    "            The input image is a jpeg-file with this file-path.\n",
    "\n",
    "        :param image:\n",
    "            The input image is a 3-dim array which is already decoded.\n",
    "            The pixels MUST be values between 0 and 255 (float or int).\n",
    "\n",
    "        :return:\n",
    "            Dict for feeding to the Inception graph in TensorFlow.\n",
    "        \"\"\"\n",
    "\n",
    "        if image is not None:\n",
    "            # Image is passed in as a 3-dim array that is already decoded.\n",
    "            feed_dict = {self.tensor_name_input_image: image}\n",
    "\n",
    "        elif image_path is not None:\n",
    "            # Read the jpeg-image as an array of bytes.\n",
    "            image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "\n",
    "            # Image is passed in as a jpeg-encoded image.\n",
    "            feed_dict = {self.tensor_name_input_jpeg: image_data}\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Either image or image_path must be set.\")\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def classify(self, image_path=None, image=None):\n",
    "        \"\"\"\n",
    "        Use the Inception model to classify a single image.\n",
    "\n",
    "        The image will be resized automatically to 299 x 299 pixels,\n",
    "        see the discussion in the Python Notebook for Tutorial #07.\n",
    "\n",
    "        :param image_path:\n",
    "            The input image is a jpeg-file with this file-path.\n",
    "\n",
    "        :param image:\n",
    "            The input image is a 3-dim array which is already decoded.\n",
    "            The pixels MUST be values between 0 and 255 (float or int).\n",
    "\n",
    "        :return:\n",
    "            Array of floats (aka. softmax-array) indicating how likely\n",
    "            the Inception model thinks the image is of each given class.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a feed-dict for the TensorFlow graph with the input image.\n",
    "        feed_dict = self._create_feed_dict(image_path=image_path, image=image)\n",
    "\n",
    "        # Execute the TensorFlow session to get the predicted labels.\n",
    "        pred = self.session.run(self.y_pred, feed_dict=feed_dict)\n",
    "\n",
    "        # Reduce the array to a single dimension.\n",
    "        pred = np.squeeze(pred)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def get_resized_image(self, image_path=None, image=None):\n",
    "        \"\"\"\n",
    "        Input an image to the Inception model and return\n",
    "        the resized image. The resized image can be plotted so\n",
    "        we can see what the neural network sees as its input.\n",
    "\n",
    "        :param image_path:\n",
    "            The input image is a jpeg-file with this file-path.\n",
    "\n",
    "        :param image:\n",
    "            The input image is a 3-dim array which is already decoded.\n",
    "            The pixels MUST be values between 0 and 255 (float or int).\n",
    "\n",
    "        :return:\n",
    "            A 3-dim array holding the image.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a feed-dict for the TensorFlow graph with the input image.\n",
    "        feed_dict = self._create_feed_dict(image_path=image_path, image=image)\n",
    "\n",
    "        # Execute the TensorFlow session to get the predicted labels.\n",
    "        resized_image = self.session.run(self.resized_image, feed_dict=feed_dict)\n",
    "\n",
    "        # Remove the 1st dimension of the 4-dim tensor.\n",
    "        resized_image = resized_image.squeeze(axis=0)\n",
    "\n",
    "        # Scale pixels to be between 0.0 and 1.0\n",
    "        resized_image = resized_image.astype(float) / 255.0\n",
    "\n",
    "        return resized_image\n",
    "\n",
    "    def print_scores(self, pred, k=10, only_first_name=True):\n",
    "        \"\"\"\n",
    "        Print the scores (or probabilities) for the top-k predicted classes.\n",
    "\n",
    "        :param pred:\n",
    "            Predicted class-labels returned from the predict() function.\n",
    "\n",
    "        :param k:\n",
    "            How many classes to print.\n",
    "\n",
    "        :param only_first_name:\n",
    "            Some class-names are lists of names, if you only want the first name,\n",
    "            then set only_first_name=True.\n",
    "\n",
    "        :return:\n",
    "            Nothing.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get a sorted index for the pred-array.\n",
    "        idx = pred.argsort()\n",
    "\n",
    "        # The index is sorted lowest-to-highest values. Take the last k.\n",
    "        top_k = idx[-k:]\n",
    "\n",
    "        # Iterate the top-k classes in reversed order (i.e. highest first).\n",
    "        for cls in reversed(top_k):\n",
    "            # Lookup the class-name.\n",
    "            name = self.name_lookup.cls_to_name(cls=cls, only_first_name=only_first_name)\n",
    "\n",
    "            # Predicted score (or probability) for this class.\n",
    "            score = pred[cls]\n",
    "\n",
    "            # Print the score and class-name.\n",
    "            print(\"{0:>6.2%} : {1}\".format(score, name))\n",
    "        return score, name\n",
    "\n",
    "    def transfer_values(self, image_path=None, image=None):\n",
    "        \"\"\"\n",
    "        Calculate the transfer-values for the given image.\n",
    "        These are the values of the last layer of the Inception model before\n",
    "        the softmax-layer, when inputting the image to the Inception model.\n",
    "\n",
    "        The transfer-values allow us to use the Inception model in so-called\n",
    "        Transfer Learning for other data-sets and different classifications.\n",
    "\n",
    "        It may take several hours or more to calculate the transfer-values\n",
    "        for all images in a data-set. It is therefore useful to cache the\n",
    "        results using the function transfer_values_cache() below.\n",
    "\n",
    "        :param image_path:\n",
    "            The input image is a jpeg-file with this file-path.\n",
    "\n",
    "        :param image:\n",
    "            The input image is a 3-dim array which is already decoded.\n",
    "            The pixels MUST be values between 0 and 255 (float or int).\n",
    "\n",
    "        :return:\n",
    "            The transfer-values for those images.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a feed-dict for the TensorFlow graph with the input image.\n",
    "        feed_dict = self._create_feed_dict(image_path=image_path, image=image)\n",
    "\n",
    "        # Use TensorFlow to run the graph for the Inception model.\n",
    "        # This calculates the values for the last layer of the Inception model\n",
    "        # prior to the softmax-classification, which we call transfer-values.\n",
    "        transfer_values = self.session.run(self.transfer_layer, feed_dict=feed_dict)\n",
    "\n",
    "        # Reduce to a 1-dim array.\n",
    "        transfer_values = np.squeeze(transfer_values)\n",
    "\n",
    "        return transfer_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NameLookup:\n",
    "    \"\"\"\n",
    "    Used for looking up the name associated with a class-number.\n",
    "    This is used to print the name of a class instead of its number,\n",
    "    e.g. \"plant\" or \"horse\".\n",
    "\n",
    "    Maps between:\n",
    "    - cls is the class-number as an integer between 1 and 1000 (inclusive).\n",
    "    - uid is a class-id as a string from the ImageNet data-set, e.g. \"n00017222\".\n",
    "    - name is the class-name as a string, e.g. \"plant, flora, plant life\"\n",
    "\n",
    "    There are actually 1008 output classes of the Inception model\n",
    "    but there are only 1000 named classes in these mapping-files.\n",
    "    The remaining 8 output classes of the model should not be used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Mappings between uid, cls and name are dicts, where insertions and\n",
    "        # lookup have O(1) time-usage on average, but may be O(n) in worst case.\n",
    "        self._uid_to_cls = {}   # Map from uid to cls.\n",
    "        self._uid_to_name = {}  # Map from uid to name.\n",
    "        self._cls_to_uid = {}   # Map from cls to uid.\n",
    "\n",
    "        # Read the uid-to-name mappings from file.\n",
    "        path = os.path.join(data_dir, path_uid_to_name)\n",
    "        with open(file=path, mode='r') as file:\n",
    "            # Read all lines from the file.\n",
    "            lines = file.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                # Remove newlines.\n",
    "                line = line.replace(\"\\n\", \"\")\n",
    "\n",
    "                # Split the line on tabs.\n",
    "                elements = line.split(\"\\t\")\n",
    "\n",
    "                # Get the uid.\n",
    "                uid = elements[0]\n",
    "\n",
    "                # Get the class-name.\n",
    "                name = elements[1]\n",
    "\n",
    "                # Insert into the lookup-dict.\n",
    "                self._uid_to_name[uid] = name\n",
    "\n",
    "        # Read the uid-to-cls mappings from file.\n",
    "        path = os.path.join(data_dir, path_uid_to_cls)\n",
    "        with open(file=path, mode='r') as file:\n",
    "            # Read all lines from the file.\n",
    "            lines = file.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                # We assume the file is in the proper format,\n",
    "                # so the following lines come in pairs. Other lines are ignored.\n",
    "\n",
    "                if line.startswith(\"  target_class: \"):\n",
    "                    # This line must be the class-number as an integer.\n",
    "\n",
    "                    # Split the line.\n",
    "                    elements = line.split(\": \")\n",
    "\n",
    "                    # Get the class-number as an integer.\n",
    "                    cls = int(elements[1])\n",
    "\n",
    "                elif line.startswith(\"  target_class_string: \"):\n",
    "                    # This line must be the uid as a string.\n",
    "\n",
    "                    # Split the line.\n",
    "                    elements = line.split(\": \")\n",
    "\n",
    "                    # Get the uid as a string e.g. \"n01494475\"\n",
    "                    uid = elements[1]\n",
    "\n",
    "                    # Remove the enclosing \"\" from the string.\n",
    "                    uid = uid[1:-2]\n",
    "\n",
    "                    # Insert into the lookup-dicts for both ways between uid and cls.\n",
    "                    self._uid_to_cls[uid] = cls\n",
    "                    self._cls_to_uid[cls] = uid\n",
    "\n",
    "    def uid_to_cls(self, uid):\n",
    "        \"\"\"\n",
    "        Return the class-number as an integer for the given uid-string.\n",
    "        \"\"\"\n",
    "\n",
    "        return self._uid_to_cls[uid]\n",
    "\n",
    "    def uid_to_name(self, uid, only_first_name=False):\n",
    "        \"\"\"\n",
    "        Return the class-name for the given uid string.\n",
    "\n",
    "        Some class-names are lists of names, if you only want the first name,\n",
    "        then set only_first_name=True.\n",
    "        \"\"\"\n",
    "\n",
    "        # Lookup the name from the uid.\n",
    "        name = self._uid_to_name[uid]\n",
    "\n",
    "        # Only use the first name in the list?\n",
    "        if only_first_name:\n",
    "            name = name.split(\",\")[0]\n",
    "\n",
    "        return name\n",
    "\n",
    "    def cls_to_name(self, cls, only_first_name=False):\n",
    "        \"\"\"\n",
    "        Return the class-name from the integer class-number.\n",
    "\n",
    "        Some class-names are lists of names, if you only want the first name,\n",
    "        then set only_first_name=True.\n",
    "        \"\"\"\n",
    "\n",
    "        # Lookup the uid from the cls.\n",
    "        uid = self._cls_to_uid[cls]\n",
    "\n",
    "        # Lookup the name from the uid.\n",
    "        name = self.uid_to_name(uid=uid, only_first_name=only_first_name)\n",
    "\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Inception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputlist=[]\n",
    "\n",
    "def classify(image_path):\n",
    "    # Display the image.\n",
    "    #display(Image(image_path))\n",
    "\n",
    "    # Use the Inception model to classify the image.\n",
    "    pred = model.classify(image_path=image_path)\n",
    "\n",
    "    # Print the scores and names for the top-10 predictions.\n",
    "    #model.print_scores(pred=pred, k=1, only_first_name=True)\n",
    "    outputlist.append(model.print_scores(pred=pred, k=1, only_first_name=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of Domestic dog sysnet\n",
    "#http://www.image-net.org/api/text/wordnet.structure.hyponym?wnid=n02084071&full=1\n",
    "\n",
    "#list of domestic cat sysnet\n",
    "#http://www.image-net.org/api/text/wordnet.structure.hyponym?wnid=n02121808&full=1\n",
    "\n",
    "#http://image-net.org/archive/words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordnet=pd.read_csv('wordnet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "flist=os.listdir('/home/rogue1/Documents/cats_dogs/train')\n",
    "\n",
    "key=[]\n",
    "\n",
    "for fl in flist:\n",
    "    image_path = '/home/rogue1/Documents/cats_dogs/train/' + fl\n",
    "    classify(image_path)\n",
    "    key.append(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output=pd.DataFrame(outputlist)\n",
    "output.columns=['prob','wnid_words']\n",
    "output=pd.merge(output,wordnet,on='wnid_words',how='left')\n",
    "output['key']=pd.DataFrame(key)\n",
    "output['key']=output['key'].str.replace('dog.','')\n",
    "output['key']=output['key'].str.replace('cat.','')\n",
    "output['key']=output['key'].str.replace('.jpg','')\n",
    "output.to_csv('output.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
